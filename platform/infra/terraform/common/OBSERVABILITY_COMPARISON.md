# Observability Setup Comparison: Accelerator vs. New Approach

## Overview
This document compares the terraform-aws-observability-accelerator module setup with our new GitOps-based approach.

---

## Component Comparison

### 1. Amazon Managed Grafana (AMG)
| Feature | Accelerator | New Approach | Status |
|---------|-------------|--------------|--------|
| Workspace Creation | âœ… Via Terraform | âœ… Via Terraform | âœ… Same |
| SAML Configuration | âœ… Configured | âœ… Configured | âœ… Same |
| API Keys | âœ… Created | âœ… Created | âœ… Same |
| Data Sources | âœ… AMP, CloudWatch, X-Ray | âœ… AMP, CloudWatch, X-Ray | âœ… Same |

### 2. Amazon Managed Prometheus (AMP)
| Feature | Accelerator | New Approach | Status |
|---------|-------------|--------------|--------|
| Workspace Creation | âœ… Via Terraform | âœ… Via Terraform | âœ… Same |
| Metrics Collection | âœ… ADOT Collector | âœ… AWS Prometheus Scraper | âœ… Different (Native AWS Service) |
| Recording Rules | âœ… Deployed via Terraform | âŒ Not implemented | âš ï¸ Optional |
| Alerting Rules | âœ… Deployed via Terraform | âŒ Not implemented | âš ï¸ Optional |

### 3. Metrics Collection Components

#### Accelerator Approach
```
ADOT Operator (Helm via Terraform)
  â”œâ”€â”€ OpenTelemetry Collector (deployed as DaemonSet/Deployment)
  â”œâ”€â”€ Scrapes metrics from pods/services
  â”œâ”€â”€ Sends to AMP via remote_write
  â””â”€â”€ Requires cert-manager (deployed by module)
```

#### New Approach
```
AWS Prometheus Scraper (Native AWS Service)
  â”œâ”€â”€ Managed by AWS (no operator needed)
  â”œâ”€â”€ Scrapes metrics directly from EKS
  â”œâ”€â”€ Sends to AMP natively
  â””â”€â”€ Uses existing cert-manager (via ArgoCD)
```

### 4. Kubernetes Components

| Component | Accelerator | New Approach | Deployment Method |
|-----------|-------------|--------------|-------------------|
| **kube-state-metrics** | âœ… Deployed via Helm | âœ… Deployed via ArgoCD | GitOps (hub-config.yaml) |
| **prometheus-node-exporter** | âœ… Deployed via Helm | âœ… Deployed via ArgoCD | GitOps (hub-config.yaml) |
| **cert-manager** | âœ… Deployed by module | âœ… Already via ArgoCD | Reused existing |
| **external-secrets** | âœ… Deployed by module | âœ… Already via ArgoCD | Reused existing |
| **grafana-operator** | âœ… Deployed by module | âœ… Already via ArgoCD | Reused existing |
| **ADOT Operator** | âœ… Deployed via Helm | âŒ Not deployed | Not needed with Scraper |
| **OpenTelemetry Collector** | âœ… Deployed via Helm | âŒ Not deployed | Not needed with Scraper |

### 5. Grafana Dashboards

| Dashboard | Accelerator | New Approach | Source |
|-----------|-------------|--------------|--------|
| Cluster Overview | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Namespace Workloads | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Node Exporter | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Nodes | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Workloads | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Kubelet | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| Java/JMX | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| NGINX | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| API Server (Basic) | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| API Server (Advanced) | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |
| API Server (Troubleshooting) | âœ… Via Grafana Operator | âœ… Via Grafana Operator | GitOps |

**Dashboard URLs**: All dashboards reference the same GitHub raw URLs from the accelerator repository.

### 6. Metrics Scraping Configuration

#### Accelerator Scrape Jobs
The accelerator deployed these via OpenTelemetry Collector configuration:
- kubernetes-apiservers
- kubernetes-nodes
- kubernetes-nodes-cadvisor
- kubelet
- kube-state-metrics
- node-exporter
- adot-collector (self-monitoring)
- custom-metrics (port 8080)
- java-jmx
- nginx-ingress
- kubernetes-service-endpoints
- kubernetes-pods
- coredns
- aws-cni-metrics
- karpenter

#### New Approach Scrape Jobs
All the same jobs are configured in `scraper-config-accelerator.yaml`:
- âœ… All accelerator jobs included
- âœ… Same metric relabeling rules
- âœ… Same filtering and dropping rules
- âœ… Custom metrics on port 8080 with unspecified prefix dropping

### 7. Secrets Management

| Secret | Accelerator | New Approach | Method |
|--------|-------------|--------------|--------|
| Grafana API Key | âœ… kubectl_manifest | âœ… ExternalSecret | GitOps (grafana-dashboards chart) |
| Grafana MySQL Creds | âŒ Not managed | âœ… ExternalSecret | GitOps (grafana-dashboards chart) |
| Storage | AWS Secrets Manager | AWS Secrets Manager | Same |

---

## What We're NOT Implementing (Optional Features)

### 1. Prometheus Recording Rules
**What they do**: Pre-compute expensive queries and store as new metrics
**Accelerator had**: 
- Infrastructure recording rules (CPU, memory aggregations)
- Java-specific recording rules
- NGINX-specific recording rules

**Why not needed now**: 
- Can be added later via Terraform if needed
- AMP supports recording rules natively
- Not critical for initial observability

### 2. Prometheus Alerting Rules
**What they do**: Define alert conditions based on metrics
**Accelerator had**:
- Node alerts (high CPU, memory, disk)
- Pod alerts (crash loops, OOM kills)
- API server alerts
- Java-specific alerts
- NGINX-specific alerts

**Why not needed now**:
- Can be added later via Terraform if needed
- AMG has unified alerting enabled
- Teams can define their own alerts in Grafana

### 3. Alert Manager Configuration
**What it does**: Routes alerts to notification channels
**Accelerator had**: Basic configuration for SNS integration

**Why not needed now**:
- AMG unified alerting handles this
- Can be configured in Grafana UI
- More flexible for teams to manage

---

## Key Differences Summary

### Advantages of New Approach

1. **Native AWS Service**: Prometheus Scraper is fully managed by AWS
   - No operator pods to maintain
   - No ADOT collector resource overhead
   - Automatic scaling and HA

2. **Reduced Complexity**: 
   - Fewer Helm releases to manage
   - No Terraform-managed Kubernetes resources
   - Leverages existing ArgoCD-deployed components

3. **GitOps Alignment**:
   - All Kubernetes resources via ArgoCD
   - Dashboards deployed declaratively
   - Easier to version control and audit

4. **Cost Optimization**:
   - No ADOT collector pods consuming cluster resources
   - Scraper runs outside cluster (AWS-managed)

5. **Separation of Concerns**:
   - Terraform: AWS resources (AMG, AMP, Scraper)
   - ArgoCD: Kubernetes resources (exporters, operators)
   - Clear boundaries

### What We Kept from Accelerator

1. âœ… All dashboard definitions (same URLs)
2. âœ… All scrape job configurations
3. âœ… Grafana Operator for dashboard management
4. âœ… External Secrets for credential management
5. âœ… Same metrics collection scope
6. âœ… Same custom metrics patterns

### What Changed

1. ğŸ”„ ADOT Collector â†’ AWS Prometheus Scraper
2. ğŸ”„ Helm deployments â†’ ArgoCD deployments
3. ğŸ”„ Terraform-managed K8s resources â†’ GitOps-managed
4. âŒ Recording rules (can add later if needed)
5. âŒ Alerting rules (can add later if needed)

---

## Configuration Files Reference

### New Approach Files
```
appmod-blueprints/
â”œâ”€â”€ platform/infra/terraform/
â”‚   â”œâ”€â”€ hub-config.yaml                          # Addon enablement
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ observability.tf                     # AMG, AMP, Scraper
â”‚       â””â”€â”€ manifests/
â”‚           â””â”€â”€ scraper-config-accelerator.yaml  # Scrape configuration
â””â”€â”€ gitops/addons/charts/grafana-dashboards/
    â””â”€â”€ templates/
        â”œâ”€â”€ observability-dashboards.yaml        # Dashboard CRs
        â”œâ”€â”€ external-secret.yaml                 # Grafana credentials
        â”œâ”€â”€ datasources.yaml                     # AMG data sources
        â””â”€â”€ grafana.yaml                         # Grafana instance CR
```

### Enabled Addons (hub-config.yaml)
```yaml
spoke-dev & spoke-prod:
  enable_kube_state_metrics: true
  enable_prometheus_node_exporter: true
  enable_prometheus_scraper: true
  enable_cni_metrics_helper: true
  enable_cert_manager: true          # Already enabled
  enable_external_secrets: true      # Already enabled
  enable_grafana_operator: true      # Already enabled (hub only)
```

---

## Migration Impact

### No Impact (Already Working)
- âœ… Existing metrics collection continues
- âœ… Existing dashboards remain functional
- âœ… Grafana access unchanged
- âœ… SAML authentication unchanged

### Improvements
- âœ… Reduced cluster resource usage (no ADOT pods)
- âœ… Simplified troubleshooting (fewer moving parts)
- âœ… Better GitOps alignment
- âœ… Easier to audit and version control

### To Add Later (If Needed)
- Recording rules for query optimization
- Alerting rules for proactive monitoring
- Alert Manager routing configuration

---

## Validation Checklist

After deployment, verify:

- [ ] AMG workspace accessible
- [ ] AMP workspace receiving metrics
- [ ] Prometheus Scraper running (check AWS console)
- [ ] kube-state-metrics pods running in clusters
- [ ] prometheus-node-exporter pods running in clusters
- [ ] Grafana dashboards visible in AMG
- [ ] Dashboards showing data from both clusters
- [ ] Custom metrics (port 8080) being scraped
- [ ] Java/JMX metrics visible (if Java apps deployed)
- [ ] NGINX metrics visible

---

## Conclusion

The new approach provides **equivalent observability** to the accelerator module while:
- Using native AWS services (Prometheus Scraper)
- Following GitOps best practices
- Reducing operational complexity
- Maintaining all dashboard and metrics collection capabilities

Recording and alerting rules can be added later via Terraform if teams require them, but the core observability stack is complete and functional.
