spark-operator:
  # Spark Operator configuration
  image:
    registry: ghcr.io
    repository: kubeflow/spark-operator/controller
    pullPolicy: IfNotPresent

  # Spark job configuration
  spark:
    jobNamespaces:
      - ""  # Watch all namespaces

  # Controller configuration
  controller:
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    livenessProbe:
      timeoutSeconds: 5
    readinessProbe:
      timeoutSeconds: 5

  # Webhook configuration
  webhook:
    enable: true
    namespaceSelector: {}
    objectSelector: {}
    timeoutSeconds: 30
    failurePolicy: Fail
    livenessProbe:
      timeoutSeconds: 5
    readinessProbe:
      timeoutSeconds: 5

  # Service accounts
  serviceAccounts:
    spark:
      create: true
      name: spark
      annotations: {}
    sparkoperator:
      create: true
      name: spark-operator
      annotations: {}

  # RBAC configuration
  rbac:
    create: true
    createClusterRole: true
    createRole: true

  # Spark History Server configuration
  sparkHistoryServer:
    enabled: true
    image:
      repository: apache/spark
      tag: 3.5.0
      pullPolicy: IfNotPresent

    # OIDC Authentication configuration
    auth:
      enabled: true
      type: oidc
      oidc:
        clientId: spark
        scopes: 'openid profile email groups'
        usernameKey: preferred_username
        groupsKey: groups

    # Environment variables for OIDC
    extraEnv:
      - name: SPARK_HISTORY_OPTS
        value: '-Dspark.history.ui.acls.enable=true -Dspark.history.ui.admin.acls=admin -Dspark.history.ui.admin.acls.groups=admin'
      - name: OAUTH_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            name: keycloak-clients
            key: SPARK_CLIENT_SECRET

    # Resource configuration
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 256Mi

    # Persistent storage for Spark event logs
    persistence:
      enabled: true
      storageClass: gp3
      size: 20Gi
      accessMode: ReadWriteOnce
      mountPath: /opt/spark/spark-events

    # Service configuration
    service:
      type: ClusterIP
      port: 18080
      targetPort: 18080

    # Ingress configuration
    ingress:
      enabled: true
      className: nginx
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/use-regex: 'true'
        nginx.ingress.kubernetes.io/proxy-body-size: '0'
        nginx.ingress.kubernetes.io/proxy-read-timeout: '600'
        nginx.ingress.kubernetes.io/proxy-send-timeout: '600'
      pathType: ImplementationSpecific

    # Security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 185
      fsGroup: 185

    # Pod security context
    podSecurityContext:
      fsGroup: 185

    # Health checks
    livenessProbe:
      httpGet:
        path: /
        port: 18080
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /
        port: 18080
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3

  # Spark application defaults
  sparkApplicationDefaults:
    # Default Spark configuration for applications
    sparkConf:
      'spark.eventLog.enabled': 'true'
      'spark.eventLog.dir': 'file:///opt/spark/spark-events'
      'spark.history.fs.logDirectory': 'file:///opt/spark/spark-events'
      'spark.kubernetes.authenticate.driver.serviceAccountName': 'spark'
      'spark.kubernetes.authenticate.executor.serviceAccountName': 'spark'
      'spark.kubernetes.container.image.pullPolicy': 'IfNotPresent'
      'spark.sql.adaptive.enabled': 'true'
      'spark.sql.adaptive.coalescePartitions.enabled': 'true'

    # Default resource configuration for Spark applications
    driver:
      cores: 1
      memory: '1g'
      serviceAccount: spark
      labels:
        version: 3.5.0
      env:
        - name: SPARK_CONF_DIR
          value: /opt/spark/conf

    executor:
      cores: 1
      instances: 2
      memory: '1g'
      labels:
        version: 3.5.0

  # Monitoring configuration
  metrics:
    enable: true
    port: 8080
    endpoint: /metrics

  # Node selector and tolerations
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod disruption budget
  podDisruptionBudget:
    enabled: false
    minAvailable: 1

  # Network policies
  networkPolicy:
    enabled: false

  # Security context for operator
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000

  # Pod security context for operator
  podSecurityContext:
    fsGroup: 1000
